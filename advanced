
#search for package
 dnf provides */mkfs.vfat

===================================== Network Manager
 https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_basic_system_settings/assembly_configuring-and-managing-network-access_configuring-basic-system-settings

sudo yum -y install NetworkManager
systemctl enable NetworkManager --now

-------------------------nmtui

ip a

Just check which IP they have and add the IP is down
Make sure to change the hostname 
Make sure to activate the connection

------------------------- nmcli

ip a

nmcli con show
nmcli device status
nmcli con show "System eth0"

#edit a interface connection
nmcli con mod "System eth0" ipv4.addresses 192.168.55.150/24 ipv4.gateway 192.168.55.1 ipv4.dns 8.8.8.8 ipv4.method manual connection.autoconnect yes
nmcli con up eth0

#configure connection
sudo nmcli con add con-name "Default1" type ethernet ifname ensl60 ipv4 129.187.345.1/24 gw4 129.187.345.0
nmcli con show
nmclli con up "Default1"
nmcli con mod "Default1" connection.autoconnect yes

to change the ipaddr
nmcli con mod Default1 ipv4.addresses 192.168.12.34/24

#add DNS
nmcli con mod Default1 ipv4.dns 172.123.456.32

#add more ip to the interface
nmcli con mod Default1 +ipv4.addresses 192.168.12.88/24


-------------------------- hostname

cat /etc/hostname
hostnamectl set-default <hostname>

------------------------ /etc/hosts
add the new ip address in all servers
IPserver1 server.example.com server1


==================================== Recover ROOT Password from boot

hold shift OR F8 
select Rescue mode
press E
in the line linux... at the end type init=/bin/bash
Ctrl + X
mount -o remount,rw /
passwd <password_2x>
touch /.autorelabel
reboot  OR exec /usr/lib/systemd/systemd

#if keep in a loop need to troubleshooting and remove the /.autorelabel at the end


================================ Boot interface

systemctl get-default

systemctl set-default

#check list for set-default
ls -l /usr/lib/systemd/system/ 


=================================================== SSH

-----------------------root ssh
yum -y install sshd
systemctl enable sshd.service --now
vi /etc/sshd/sshd_config
  PermitRootLogin yes
systemctl restart sshd.service 

----------------------ssh root key pair
vi /etc/sshd/sshd_config
    PubkeyAuthentication yes  
ssh-keygen
cd ~/.ssh
ssh-copy-id root@server2:/root/.ssh
The plublic key it will be in ~/.ssh/authorized_keys

ssh-keygen
cd ~/.ssh
ssh-copy-id root@server1:/root/.ssh












============================== Containers

sudo yum -y install container-tools


------------------ Auto-starting Rootless Containers
1 - need a non-root user acc
  useradd <username>
  passwd <username>

2- enable linger
What is enable linger?
If enabled for a specific user, a user manager is spawned for the user at boot and kept around after logouts. 
This allows users who are not logged in to run long-running services.

  loginctl enable-linger <username>
  loginctl show-user linda        # verify Linger=yes

3 - Need to ssh to the user. It needs to be a real login, su doesn't work.
  ssh <username>@localhost
  
4 - Create a container
  podman run -d --name mynginx -p 8081:80 docker.io/library/nginx

5 - Generate a systemd unit file to autostart the container
   mkdir -p ~/.config/systemd/user
   cd ~/.config/systemd/user
   podman generate systemd --name mynginx --files
   cat container-mynginx.service          #check the content
   systemctl --user daemon-reload
   
6 - automatically start the user container on boot
  podman stop mynginx
  systemctl --user enable --now container-mynginx.service
  systemctl --user status container-mynginx.service       # verify
  podman ps                                                     #verify the container
  
  
================================== Permissions

chmod 770 /path/{dir1,dir2}

you can only delete the file if you are the owner of the file, or the owner of the directory that contains the file.
when there is a stickybit!


================================= Managing users and groups
The main file used when users are created is /etc/login.defs. In this file many - but not all - default settings are defined for new user accounts.

================================== Managing Processes , NICE, RENICE

type & in the end
jobs command for check process in the background
fg <number> bring back the process to foreground
ctrl+z to pause the process 
bg to continue processes paused

----- Change process priority
start a process with different priprity
nice -n -8 dd if=/dev/zero of=/dev/null &


======================================= Check package

dnf provides */killall


========================================== SELinux

a package that is needed to do decent SELinux troubleshooting
yum -y install setroubleshoot-server

------------------Managing SELinux File Context

vi /etc/httpd/conf/httpd.conf
change DocumentRoot
<Directory>

SELinux messages are written with the audit label AVC: 
grep AVC /var/log/audit/audit.log | grep index.html

In some cases it's also useful to see if sealert has generated output. 
You'll find that in journalctl output: journalctl | grep sealert

ls -Z /var/www
ls -Z /web
semanage fcontext -a -t httpd_sys_content_t "/web(/.*)?"
restorecon -Rv /web


=================================== NFS ======== AUTOMOUNT


------------------ NFS server
dnf install -y nfs-utils

In NFS you'll need to share something. Let's create a dummy user homedirectory structure which can be shared next: 
mkdir -p /users/nfs/{lisa,linda,anna}

 tell NFS to share the directory containing these user home directories. To do so, it needs a file /etc/exports 
 that contains the name of the directory to share, including some access permissions: 
 echo "/users *(rw,no_root_squash)" > /etc/exports

Enable and verify the NFS server
systemctl enable --now nfs-server

The showmount command provides an easy way to test NFS server access. 
In this case it's super-easy, as the NFS server isn't even running remotely. 
That's why you don't have to configure any firewall either. Use the following command to verify the NFS shares are available: 

showmount -e localhost
Export list for localhost:
/users *

You'll see that the directory /users is exported, which means that it's time to take care of automount setup.

To use automount, you need to install the autofs service.
As autofs doesn't really have any configuration yet, it doesn't make much sense to start it at this point already
dnf install -y autofs


Create the automount configuration
The automount configuration is stored in two files. 
The /etc/auto.master file is used to identify the directory that automount will manage, as well as the file that is used for further configuration.

to add a line to the auto.master file: 
echo "/home/nfs  /etc/auto.nfs" >> /etc/auto.master   
   
   
You need wildcards to refer to any directory, and to refer to the matching part on the NFS server, you need an ampersand. 
So to create the automount configuration, use 
echo "*   -rw   localhost:/users/nfs/&" > /etc/auto.nfs

This completes the configuration, so it's time to start and enable the autofs service: 
systemctl enable --now autofs

Verify the mounts
ls -al /home/nfs

In the previous step you've seen that the /home/nfs directory didn't contain any subdirectories. That is expected, because autofs will only become active when one of the subdirectories is actually accessed. 
For a home directory, that would happen automatically when the user is logging in. Use the following to simulate this: 
cd /home/nfs/linda

Type pwd to verify that you are indeed in the directory /home/nfs/linda, which has now been automounted for you.

To understand a bit more about all that is happening in the background, use 
mount | tail -5 
where you'll see the mounts that have been created by autofs.




